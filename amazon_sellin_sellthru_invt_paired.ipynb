{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "numerical-border",
   "metadata": {},
   "source": [
    "# Code created for Report in following doc:\n",
    "G:\\SALES\\2022 Sales Reports\\Sell-Through Reporting\\Amazon\\Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "quality-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the appropriate libraries we'll need for this code\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime \n",
    "from datetime import timedelta,datetime as dt\n",
    "import datetime as dt\n",
    "import pyodbc\n",
    "import glob\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "negative-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL python connection to our server\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=sql-2-db;'\n",
    "                      'Database=CBQ2;')\n",
    "                    \n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "employed-revision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What week number are we in? 46\n"
     ]
    }
   ],
   "source": [
    "# Figure out the dates for the weekly data\n",
    "weeknum = int(input(\"What week number are we in? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-politics",
   "metadata": {},
   "source": [
    "# Creating dates for the first week of the year and the current rolling week in a string format for the sql query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "standing-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the week 1 dates in the Amazon files for each year set in python datetime format.\n",
    "ty_wk1 = datetime.date(2022,1,8)\n",
    "ly_wk1 = datetime.date(2021,1,9)\n",
    "py_wk1 = datetime.date(2020,1,4)\n",
    "ppy_wk1 = datetime.date(2019,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "defined-turner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty = ty_wk1.year\n",
    "\n",
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "simple-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the corresponding week date for each year\n",
    "current_ty = ty_wk1 + timedelta(weeks = weeknum-1)\n",
    "current_ly = ly_wk1 + timedelta(weeks = weeknum-1)\n",
    "current_py = py_wk1 + timedelta(weeks = weeknum-1)\n",
    "current_ppy = ppy_wk1 + timedelta(weeks = weeknum-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "spectacular-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets week 1 dates as a string format for the sql query\n",
    "date_ty_wk1_str = ty_wk1.strftime(\"%Y-%m-%d\")\n",
    "date_ly_wk1_str = ly_wk1.strftime(\"%Y-%m-%d\")\n",
    "date_py_wk1_str = py_wk1.strftime(\"%Y-%m-%d\")\n",
    "date_ppy_wk1_str = ppy_wk1.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "continuous-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets current week date as a string format for the sql query\n",
    "date_ty_current_str = current_ty.strftime(\"%Y-%m-%d\")\n",
    "date_ly_current_str = current_ly.strftime(\"%Y-%m-%d\")\n",
    "date_py_current_str = current_py.strftime(\"%Y-%m-%d\")\n",
    "date_ppy_current_str = current_ppy.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-fellow",
   "metadata": {},
   "source": [
    "# Creating dates 10 weeks past to properly calculate the weeks on hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "thorough-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates 10 weeks into the past\n",
    "date_current_ty_minus10 = current_ty + timedelta(weeks = -9)\n",
    "date_current_ly_minus10 = current_ly + timedelta(weeks = -9)\n",
    "date_current_py_minus10 = current_py + timedelta(weeks = -9)\n",
    "date_current_ppy_minus10 = current_ppy + timedelta(weeks = -9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fluid-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_current_ty_minus10_str = date_current_ty_minus10.strftime(\"%Y-%m-%d\")\n",
    "date_current_ly_minus10_str = date_current_ly_minus10.strftime(\"%Y-%m-%d\")\n",
    "date_current_py_minus10_str = date_current_py_minus10.strftime(\"%Y-%m-%d\")\n",
    "date_current_ppy_minus10_str = date_current_ppy_minus10.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-trustee",
   "metadata": {},
   "source": [
    "# Creating the period strings for the first month and current month for the SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "polished-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the corresponding \"period\" for week1 of each year\n",
    "period_ty_wk1 = ty_wk1.strftime(\"%Y%m\")\n",
    "period_ly_wk1 = ly_wk1.strftime(\"%Y%m\")\n",
    "period_py_wk1 = py_wk1.strftime(\"%Y%m\")\n",
    "period_ppy_wk1 = ppy_wk1.strftime(\"%Y%m\")\n",
    "\n",
    "#Calculates the corresponding \"period\" for current week of each year\n",
    "period_ty_current = current_ty.strftime(\"%Y%m\")\n",
    "period_ly_current = current_ly.strftime(\"%Y%m\")\n",
    "period_py_current = current_py.strftime(\"%Y%m\")\n",
    "period_ppy_current = current_ppy.strftime(\"%Y%m\")\n",
    "\n",
    "#Calculates the corresponding \"period\" for the week 10 weeks prior to the current \n",
    "period_ty_minus10 = date_current_ty_minus10.strftime(\"%Y%m\")\n",
    "period_ly_minus10 = date_current_ly_minus10.strftime(\"%Y%m\")\n",
    "period_py_minus10 = date_current_py_minus10.strftime(\"%Y%m\")\n",
    "period_ppy_minus10 = date_current_ppy_minus10.strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-bidding",
   "metadata": {},
   "source": [
    "# List of dates and periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "reliable-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates\n",
    "ls_date_wk1 = list([ty_wk1,ly_wk1,py_wk1,ppy_wk1])\n",
    "ls_date_current_wk = list([current_ty,current_ly,current_py,current_ppy])\n",
    "ls_date_wk1_str = list([date_ty_wk1_str,date_ly_wk1_str,date_py_wk1_str,date_ppy_wk1_str])\n",
    "ls_date_current_str = list([date_ty_current_str,date_ly_current_str,date_py_current_str,date_ppy_current_str])\n",
    "ls_date_current_minus10_str = list([date_current_ty_minus10_str,date_current_ly_minus10_str,date_current_py_minus10_str,date_current_ppy_minus10_str])\n",
    "\n",
    "# Periods\n",
    "ls_period_wk1 = list([period_ty_wk1,period_ly_wk1,period_py_wk1,period_ppy_wk1])\n",
    "ls_period_current = list([period_ty_current,period_ly_current,period_py_current,period_ppy_current])\n",
    "ls_period_minus10 = list([period_ty_minus10,period_ly_minus10,period_py_minus10,period_ppy_minus10])\n",
    "\n",
    "# Basic\n",
    "ls_years = list(['ty','ly','py','ppy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "second-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-11-19', '2021-11-20', '2020-11-14', '2019-11-23']\n",
      "['2022-09-17', '2021-09-18', '2020-09-12', '2019-09-21']\n",
      "['202209', '202109', '202009', '201909']\n"
     ]
    }
   ],
   "source": [
    "print(ls_date_current_str)\n",
    "print(ls_date_current_minus10_str)\n",
    "print(ls_period_minus10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-digit",
   "metadata": {},
   "source": [
    "# Sell-In Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "handed-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TY sellin query\n",
    "query_sellin = f'''select\n",
    "    t.ITEM_TITLE ISBN\n",
    "    ,sum(sd.QUANTITY_INVOICED) qty\n",
    "    ,sum(sd.REVENUE_AMOUNT) val\n",
    "from\n",
    "    ebs.sales as sd\n",
    "    inner join ssr.SalesSSRRow stie on stie.CUSTOMER_TRX_LINE_ID = sd.CUSTOMER_TRX_LINE_ID\n",
    "    inner join ssr.SSRRow ssr_row on ssr_row.SSRRowID= stie.SSRRowID  \n",
    "    inner join ebs.Item as t on sd.ITEM_ID=t.ITEM_ID\n",
    "where\n",
    "    sd.PERIOD between ? and ?\n",
    "    and sd.TRX_DATE <= ?\n",
    "    and ssr_row.SSRRowID ='6'\n",
    "    and t.PRODUCT_TYPE in ('bk', 'ft')\n",
    "    and t.PUBLISHER_CODE not in('Benefit','AFO LLC','Glam Media','PQ Blackwell')\n",
    "group by\n",
    "    t.ITEM_TITLE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dressed-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTD Sell-Thru Data\n",
    "\n",
    "ls_df_sellin = [pd.read_sql_query(query_sellin,conn,params = [w,c,d])\\\n",
    "                                 for w,c,d in zip(ls_period_wk1,ls_period_current,ls_date_current_str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-series",
   "metadata": {},
   "source": [
    "   # Item table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "alleged-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_item = '''\n",
    "SELECT\n",
    "    i.PUBLISHER_CODE Publisher\n",
    "    ,i.PRODUCT_TYPE pt\n",
    "    ,i.REPORTING_CATEGORY cat\n",
    "    ,i.PUBLISHING_GROUP pgrp\n",
    "    ,i.ITEM_TITLE ISBN\n",
    "    ,i.SHORT_TITLE title\n",
    "    ,i.PRICE_AMOUNT price\n",
    "    ,i.AMORTIZATION_DATE pub\n",
    "    ,case\n",
    "        when i.AMORTIZATION_DATE is not null then year(i.AMORTIZATION_DATE)\n",
    "        when substring(i.season,1,4) <> 'No S' then substring(i.season,1,4)\n",
    "        else year(getdate())\n",
    "    end [year]\n",
    "FROM ebs.Item i\n",
    "WHERE i.PRODUCT_TYPE in ('bk', 'ft')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "spectacular-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item = pd.read_sql_query(query_item,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-preview",
   "metadata": {},
   "source": [
    "# Query for OnHand & CustomerOrders from the Sellthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "grateful-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_onhand_cust = '''\n",
    "SELECT\n",
    "    st.ISBN\n",
    "    ,sum(CustomerOrders) [CustomerOrders]\n",
    "    ,sum(OnHand) [OnHand]\n",
    "FROM\n",
    "    [CBQ2].[cb].[Sellthrough_Amazon] st\n",
    "        inner join ebs.Item i on i.ISBN = st.ISBN\n",
    "WHERE\n",
    "    st.[week] = ?\n",
    "    AND i.PRODUCT_TYPE in ('bk', 'ft')\n",
    "GROUP BY\n",
    "    st.ISBN\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "diagnostic-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df_onhand = [pd.read_sql_query(query_onhand_cust,conn,params = [x]) for x in ls_date_current_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "textile-confidentiality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-11-19', '2021-11-20', '2020-11-14', '2019-11-23']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_date_current_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "german-column",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-11-19', '2021-11-20', '2020-11-14', '2019-11-23']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_date_current_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-citizenship",
   "metadata": {},
   "source": [
    "# Sell-thru Queries and dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "therapeutic-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sellthru = '''\n",
    "SELECT\n",
    "    st.[ISBN]\n",
    "    ,sum([UnitShipped]) [ytd]\n",
    "FROM\n",
    "    [CBQ2].[cb].[Sellthrough_Amazon] st\n",
    "        inner join ebs.item i on i.ISBN = (case\n",
    "                                            when len(st.ISBN)=12 then '0' + st.ISBN\n",
    "                                            when len(st.ISBN)=11 then '00' + st.ISBN\n",
    "                                            else st.ISBN end)\n",
    "WHERE\n",
    "    st.[week] between ? and ?\n",
    "    AND i.PRODUCT_TYPE in ('bk', 'ft')\n",
    "GROUP BY\n",
    "    st.[ISBN]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "moderate-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df_sellthru = [pd.read_sql_query(query_sellthru,conn,params = [w,c])\n",
    "                  for w,c in zip(ls_date_wk1_str,ls_date_current_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "direct-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2022-01-08 2022-11-19\n",
      "2021-01-09 2021-11-20\n",
      "2020-01-04 2020-11-14\n",
      "2019-01-12 2019-11-23\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_df_sellthru))\n",
    "\n",
    "for w,c in zip(ls_date_wk1_str,ls_date_current_str):\n",
    "    print(w,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-mobile",
   "metadata": {},
   "source": [
    "# Sell-thru Query between current and 10 weeks ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "joined-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-17 2022-11-19\n",
      "2021-09-18 2021-11-20\n",
      "2020-09-12 2020-11-14\n",
      "2019-09-21 2019-11-23\n"
     ]
    }
   ],
   "source": [
    "# 10 Week Grouping\n",
    "for w,c in zip(ls_date_current_minus10_str,ls_date_current_str):\n",
    "    print(w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "pleasant-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sell-Thru 10 weeks\n",
    "ls_df_10wks_sellthru = [pd.read_sql_query(query_sellthru,conn,params = [w,c])\n",
    "                  for w,c in zip(ls_date_current_minus10_str,ls_date_current_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "allied-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ls_df_10wks_sellthru:\n",
    "    df.columns = ['ISBN','wks10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-quality",
   "metadata": {},
   "source": [
    "# Add leading zeroes to the sellthru/onhand ISBN columns with less than 13 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "sharing-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds a zero before isbn's that have less than 13 characters\n",
    "def isbn_leadzero(df):\n",
    "    df['ISBN'] = df['ISBN'].apply(lambda x: '{0:0>13}'.format(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "destroyed-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df_sellin = [isbn_leadzero(df) for df in ls_df_sellin]\n",
    "ls_df_sellthru = [isbn_leadzero(df) for df in ls_df_sellthru]\n",
    "ls_df_onhand = [isbn_leadzero(df) for df in ls_df_onhand]\n",
    "ls_df_sellthru_minus10 = [isbn_leadzero(df) for df in ls_df_10wks_sellthru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "retired-magazine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>CustomerOrders</th>\n",
       "      <th>OnHand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5055923712566</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5055923712603</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5055923712610</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5055923747087</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5055923751909</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8905</th>\n",
       "      <td>9791036313578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8906</th>\n",
       "      <td>9791036313592</td>\n",
       "      <td>45.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8907</th>\n",
       "      <td>9791036314940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908</th>\n",
       "      <td>9791036339172</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8909</th>\n",
       "      <td>9791036348822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8910 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISBN  CustomerOrders  OnHand\n",
       "0     5055923712566             8.0       4\n",
       "1     5055923712603            28.0      47\n",
       "2     5055923712610            17.0      60\n",
       "3     5055923747087            13.0       3\n",
       "4     5055923751909             4.0       3\n",
       "...             ...             ...     ...\n",
       "8905  9791036313578             0.0       8\n",
       "8906  9791036313592            45.0      12\n",
       "8907  9791036314940             0.0       5\n",
       "8908  9791036339172             3.0      39\n",
       "8909  9791036348822             0.0      13\n",
       "\n",
       "[8910 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_df_onhand[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-graphics",
   "metadata": {},
   "source": [
    "# Creating yearly tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "hungry-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a loop that goes through the 3 sets of lists (sellin,sellthru, onhand)\n",
    "# and creates yearly dataframes and puts them in this list below called, \"df_yearly\"\n",
    "\n",
    "df_yearly = list()\n",
    "\n",
    "for sellin, sellthru,minus,onhand in zip(ls_df_sellin,ls_df_sellthru,ls_df_sellthru_minus10,ls_df_onhand):\n",
    "    df_year = pd.merge(sellin,sellthru,how = 'outer',on = 'ISBN').fillna(0)\n",
    "    df_year = pd.merge(df_year,minus.loc[:,['ISBN','wks10']],how = 'outer',on='ISBN').fillna(0)\n",
    "    df_year = pd.merge(df_year,onhand.loc[:,['ISBN','OnHand']],how = 'outer',on='ISBN').fillna(0)\n",
    "    \n",
    "    filt1 = (df_year.qty==0) & (df_year.val==0) & (df_year.ytd==0) & (df_year.OnHand==0)\n",
    "    df_year = df_year.loc[~filt1]\n",
    "    \n",
    "    column_names = {'qty':'sellin_qty','val':'sellin_val','ytd':'sellthru_qty_ytd','wks10':'last10wks','OnHand':'sellthru_OnHand'}\n",
    "    df_year.rename(columns = column_names,inplace=True)\n",
    "    \n",
    "    df_year = pd.merge(df_year,df_item,how = 'left',on='ISBN')\n",
    "    \n",
    "    # Note that these titles are not in the ebs.item list are mysteriously show up in our feed.\n",
    "    filt2 = df_year.Publisher.isnull()\n",
    "    df_year = df_year.loc[~filt2]\n",
    "    \n",
    "    df_year = df_year.loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title','price',\\\n",
    "                         'pub','year', 'sellin_qty','sellin_val', 'sellthru_qty_ytd','last10wks', 'sellthru_OnHand']]\n",
    "    \n",
    "    df_yearly.append(df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-hybrid",
   "metadata": {},
   "source": [
    "# Creating unique isbn list for combo page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "completed-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_isbns = pd.concat(df_yearly)\n",
    "df_combo_isbns = df_combo_isbns.loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title','price','pub']]\n",
    "df_combo_isbns.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "billion-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first Sellin Units Section along with the variance Section\n",
    "df_combo_si = df_combo_isbns.copy()\n",
    "\n",
    "for df in ls_df_sellin:\n",
    "    df_combo_si = pd.merge(df_combo_si,df.loc[:,['ISBN','qty']],how = 'left',on = 'ISBN').fillna(0)\n",
    "\n",
    "df_combo_si.columns = ['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_si', 'ly_si', 'py_si', 'ppy_si']\n",
    "\n",
    "df_combo_si['ty_ly_si'] = ((df_combo_si['ty_si']-df_combo_si['ly_si'])/df_combo_si['ly_si']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_si['ty_py_si'] = ((df_combo_si['ty_si']-df_combo_si['py_si'])/df_combo_si['py_si']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_si['ty_ppy_si'] = ((df_combo_si['ty_si']-df_combo_si['ppy_si'])/df_combo_si['ppy_si']).replace([np.inf,-np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "square-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first Sellthru Units Section along with the variance Section\n",
    "df_combo_st = df_combo_isbns.copy()\n",
    "\n",
    "for df in ls_df_sellthru:\n",
    "    df_combo_st = pd.merge(df_combo_st,df.loc[:,['ISBN','ytd']],how = 'left',on = 'ISBN').fillna(0)\n",
    "\n",
    "df_combo_st.columns = ['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_st', 'ly_st', 'py_st', 'ppy_st']\n",
    "\n",
    "df_combo_st['ty_ly_st'] = ((df_combo_st['ty_st']-df_combo_st['ly_st'])/df_combo_st['ly_st']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_st['ty_py_st'] = ((df_combo_st['ty_st']-df_combo_st['py_st'])/df_combo_st['py_st']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_st['ty_ppy_st'] = ((df_combo_st['ty_st']-df_combo_st['ppy_st'])/df_combo_st['ppy_st']).replace([np.inf,-np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "literary-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the first OnHand Units Section along with the variance Section\n",
    "df_combo_oh = df_combo_isbns.copy()\n",
    "\n",
    "for df in ls_df_onhand:\n",
    "    df_combo_oh = pd.merge(df_combo_oh,df.loc[:,['ISBN','OnHand']],how = 'left',on = 'ISBN').fillna(0)\n",
    "\n",
    "df_combo_oh.columns = ['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_oh', 'ly_oh', 'py_oh', 'ppy_oh']\n",
    "\n",
    "df_combo_oh['ty_ly_oh'] = ((df_combo_oh['ty_oh']-df_combo_oh['ly_oh'])/df_combo_oh['ly_oh']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_oh['ty_py_oh'] = ((df_combo_oh['ty_oh']-df_combo_oh['py_oh'])/df_combo_oh['py_oh']).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "df_combo_oh['ty_ppy_oh'] = ((df_combo_oh['ty_oh']-df_combo_oh['ppy_oh'])/df_combo_oh['ppy_oh']).replace([np.inf,-np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "printable-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to join the 3 sections above to create the combo tab\n",
    "df_combo_tab = pd.merge(df_combo_si,df_combo_st,how='left',\\\n",
    "                        on=['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub'])\n",
    "\n",
    "df_combo_tab = pd.merge(df_combo_tab,df_combo_oh,how='left',\\\n",
    "                        on=['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "sealed-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the combo tab so it's descending by Sell in\n",
    "df_combo_tab.sort_values('ty_si',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-apparatus",
   "metadata": {},
   "source": [
    "# Top Deltas Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "together-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the copy of the combo tab and looking at the variances between this year and last year - sellin - sellthru\n",
    "df_combo_tab_var = df_combo_tab.copy()\n",
    "\n",
    "df_combo_tab_var['TY_vs_LY_Sellin'] = df_combo_tab_var['ty_si'] - df_combo_tab_var['ly_si']\n",
    "df_combo_tab_var['TY_vs_LY_SellThru'] = df_combo_tab_var['ty_st'] - df_combo_tab_var['ly_st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "provincial-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Publisher  pt cat pgrp           ISBN                  title  price  \\\n",
      "457  Chronicle  BK  FO  FWN  9781452179612  From Crook to Cook hc  24.95   \n",
      "\n",
      "                     pub     ty_si    ly_si    py_si   ppy_si  ty_ly_si  \\\n",
      "457  2018-10-01 00:00:00  122396.0  95217.0  96975.0  35735.0  0.285443   \n",
      "\n",
      "     ty_py_si  ty_ppy_si     ty_st    ly_st    py_st   ppy_st  ty_ly_st  \\\n",
      "457   0.26214   2.425101  115176.0  91694.0  65303.0  23565.0  0.256091   \n",
      "\n",
      "     ty_py_st  ty_ppy_st   ty_oh    ly_oh    py_oh  ppy_oh  ty_ly_oh  \\\n",
      "457  0.763717   3.887588  8158.0  23134.0  12485.0  8441.0 -0.647359   \n",
      "\n",
      "     ty_py_oh  ty_ppy_oh  TY_vs_LY_Sellin  TY_vs_LY_SellThru  \n",
      "457 -0.346576  -0.033527          27179.0            23482.0  \n",
      "Index(['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
      "       'ty_si', 'ly_si', 'py_si', 'ppy_si', 'ty_ly_si', 'ty_py_si',\n",
      "       'ty_ppy_si', 'ty_st', 'ly_st', 'py_st', 'ppy_st', 'ty_ly_st',\n",
      "       'ty_py_st', 'ty_ppy_st', 'ty_oh', 'ly_oh', 'py_oh', 'ppy_oh',\n",
      "       'ty_ly_oh', 'ty_py_oh', 'ty_ppy_oh', 'TY_vs_LY_Sellin',\n",
      "       'TY_vs_LY_SellThru'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Check a row of numbers\n",
    "print(df_combo_tab_var[df_combo_tab_var['ISBN']=='9781452179612'])\n",
    "print(df_combo_tab_var.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "rising-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the year to this df so I can filter below\n",
    "df_combo_isbns_yr = pd.concat(df_yearly)\n",
    "df_combo_isbns_yr = df_combo_isbns_yr.loc[:,['ISBN','year']]\n",
    "df_combo_isbns_yr.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "apart-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_tab_var = pd.merge(df_combo_tab_var,df_combo_isbns_yr,on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "affiliated-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sell Thru\n",
    "# Creating the top 12 CHL variances\n",
    "filt_chl = (df_combo_tab_var['pgrp'] == 'CHL') & (df_combo_tab_var['year'] != 2022)\n",
    "\n",
    "df_chl_21_22_st = df_combo_tab_var.loc[filt_chl].sort_values('TY_vs_LY_SellThru')\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_st', 'ly_st','TY_vs_LY_SellThru']]\n",
    "df_chl_22_21_st = df_combo_tab_var.loc[filt_chl].sort_values('TY_vs_LY_SellThru',ascending=False)\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_st', 'ly_st','TY_vs_LY_SellThru']]\n",
    "\n",
    "# Creating the top 12 LIF variances\n",
    "filt_lif = (df_combo_tab_var['pgrp'] == 'LIF') & (df_combo_tab_var['year'] != 2022)\n",
    "\n",
    "df_lif_21_22_st = df_combo_tab_var.loc[filt_lif].sort_values('TY_vs_LY_SellThru')\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_st', 'ly_st','TY_vs_LY_SellThru']]\n",
    "df_lif_22_21_st = df_combo_tab_var.loc[filt_lif].sort_values('TY_vs_LY_SellThru',ascending=False)\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_st', 'ly_st','TY_vs_LY_SellThru']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "future-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
       "       'ty_si', 'ly_si', 'py_si', 'ppy_si', 'ty_ly_si', 'ty_py_si',\n",
       "       'ty_ppy_si', 'ty_st', 'ly_st', 'py_st', 'ppy_st', 'ty_ly_st',\n",
       "       'ty_py_st', 'ty_ppy_st', 'ty_oh', 'ly_oh', 'py_oh', 'ppy_oh',\n",
       "       'ty_ly_oh', 'ty_py_oh', 'ty_ppy_oh', 'TY_vs_LY_Sellin',\n",
       "       'TY_vs_LY_SellThru', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combo_tab_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "unsigned-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sell In\n",
    "# Creating the top 12 CHL variances\n",
    "filt_chl = (df_combo_tab_var['pgrp'] == 'CHL') & (df_combo_tab_var['year'] != ty_wk1.year)\n",
    "\n",
    "df_chl_21_22_si = df_combo_tab_var.loc[filt_chl].sort_values('TY_vs_LY_Sellin')\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_si', 'ly_si','TY_vs_LY_Sellin']]\n",
    "df_chl_22_21_si = df_combo_tab_var.loc[filt_chl].sort_values('TY_vs_LY_Sellin',ascending=False)\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_si', 'ly_si','TY_vs_LY_Sellin']]\n",
    "\n",
    "# Creating the top 12 LIF variances\n",
    "filt_lif = (df_combo_tab_var['pgrp'] == 'LIF') & (df_combo_tab_var['year'] != ty_wk1.year)\n",
    "\n",
    "df_lif_21_22_si = df_combo_tab_var.loc[filt_lif].sort_values('TY_vs_LY_Sellin')\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_si', 'ly_si','TY_vs_LY_Sellin']]\n",
    "df_lif_22_21_si = df_combo_tab_var.loc[filt_lif].sort_values('TY_vs_LY_Sellin',ascending=False)\\\n",
    "    .head(12).loc[:,['Publisher', 'pt', 'cat', 'pgrp', 'ISBN', 'title', 'price', 'pub',\n",
    "       'ty_si', 'ly_si','TY_vs_LY_Sellin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "latin-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_deltas = [df_chl_22_21_st,df_chl_21_22_st,df_lif_22_21_st,df_lif_21_22_st,df_chl_22_21_si,df_chl_21_22_si,df_lif_22_21_si,df_lif_21_22_si]\n",
    "\n",
    "def add_row(df):\n",
    "    df['row_num'] = np.arange(len(df))+1\n",
    "    return df\n",
    "\n",
    "for ls in ls_deltas:\n",
    "    add_row(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dress-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_delta_st = pd.concat([df_chl_22_21_st,df_chl_21_22_st,df_lif_22_21_st,df_lif_21_22_st])\n",
    "df_top_delta_si = pd.concat([df_chl_22_21_si,df_chl_21_22_si,df_lif_22_21_si,df_lif_21_22_si])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-meditation",
   "metadata": {},
   "source": [
    "# Save off to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bridal-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'G:\\SALES\\2022 Sales Reports\\Sell-Through Reporting\\Amazon\\Analysis\\py_drop\\amaz_sellin_thru_py.xlsx'\n",
    "writer = pd.ExcelWriter(path, engine='xlsxwriter')\n",
    "\n",
    "for df,y in zip(df_yearly,ls_years):\n",
    "    df.to_excel(writer,sheet_name=y,index = False)\n",
    "\n",
    "df_combo_tab.to_excel(writer,sheet_name=\"Combo_tab\",index = False)\n",
    "df_top_delta_st.to_excel(writer,sheet_name=\"Delta_st\",index = False)\n",
    "df_top_delta_si.to_excel(writer,sheet_name=\"Delta_si\",index = False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-bowling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
